{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traitement d'un jeu d'articles de recherche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"../pdf\" #Spécification du chemin d'accès. peut être amené à changer selon l'espace de travail\n",
    "print(os.listdir(filepath)) #On affiche la liste de tous les fichiers présents dans le dossier, pour vérifier que notre programme les trouve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le module PDFLoader nous retourne une liste de documents, chacun contenant une chaîne de caractères par page ainsi que les métadonnées du document ddans un dictionnaire. Il faut donc rassembler toutes les pages de l'article dans la même chaîne de caractères"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = {}\n",
    "docs = {}\n",
    "\n",
    "#On utilise des dictionnaires pour nos variables afin de pouvoir récupérer toutes les données d'un document à l'aide de son nom\n",
    "\n",
    "for i in os.listdir(filepath): #Execute les instructions pour chaque fichier \n",
    "    loader = PyPDFLoader(filepath+'/'+i)\n",
    "    docs[i] = loader.load() \n",
    "    texts[i] = \"\"\n",
    "    for doc in docs[i]:\n",
    "        texts[i] += doc.page_content #Concaténation\n",
    "    texts[i] = texts[i].replace(\"\\n\", \" \") #Mise en forme en un paragraphe\n",
    "    print(i,\" - Number of pages:\", len(docs[i]),\"  Number of characters:\", len(texts[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les caractères représentant la nouvelle ligne et le nouveau paragraphe sont tous les deux \"\\n\". Il est sans doutes plus judicieux de traiter le pdf comme un énorme paragraphe pour le traitement qui va suivre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Découpage des textes en plusieurs chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 5000  # Maximum size of chunks\n",
    "chunk_overlap = 200  # Overlap in characters between chunks\n",
    "\n",
    "separators = [\".\", \" \", \"\"]\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    "    separators=separators,\n",
    "    keep_separator=False, \n",
    ") #Instanciation du \"découpeur\" des textes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = {} #Création d'une liste de chunks par texte\n",
    "for i in texts.keys():\n",
    "    splits[i] = text_splitter.split_text(texts[i])\n",
    "    print(i,\" - Number of text chunks:\", len(splits[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Création de la collection Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = chromadb.Client(Settings(chroma_db_impl=\"duckdb+parquet\",\n",
    "                                    persist_directory=\"db/\"\n",
    "                                )) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si vous voulez récupérer une collection déjà existente, éxecutez la cellule ci-dessous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = client.get_or_create_collection(name=\"Articles\") #Création de la base de données vectorielle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si vous voulez recréer une base de données vectorielle, éxecutez la cellule ci-dessous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.delete_collection(name=\"Articles\")\n",
    "collection = client.create_collection(name=\"Articles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonction permettant d'associer à quelle(s) page(s) se trouve les chunks créés plus tôt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sources(nom,splits,doc,text):\n",
    "    output = []\n",
    "    S = len(doc[0].page_content)-1\n",
    "    pages = []\n",
    "    pages.append(S)\n",
    "    for page in doc[1::]:\n",
    "        S += len(page.page_content)\n",
    "        pages.append(S)\n",
    "    for i in splits:\n",
    "        ind = text.find(i)\n",
    "        d = 0\n",
    "        f = 0\n",
    "        while pages[d] < ind : d+=1\n",
    "        while pages[f] < (ind+len(i)-1) : f+=1\n",
    "        output.append({\"source\": f\"{nom} pages {d+1}-{f+1}\"})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ajout des articles à la collection s'ils n'y sont pas déjà (cas d'une collection récupérée)\n",
    "La source et la page de chaque chunk sont stockés dans les métadonnées de chaque vecteur, à la clé \"source\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in splits.keys():\n",
    "    try :\n",
    "        collection.add(\n",
    "            documents=splits[i],\n",
    "            metadatas = sources(i,splits[i],docs[i],texts[i]),\n",
    "            ids = [f\"{i}_{j}\" for j in range(len(splits[i]))]\n",
    "        )\n",
    "        print(f\"{i} ajouté avec succès\")\n",
    "    except :\n",
    "        print(\"Cet article est déjà dans la collection. Si vous avez changé les paramètres de tokenisation, merci de supprimer la collection et d'en créer une nouvelle.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test pour vérifier la présence de chunks dans la base de données vectorielle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = 52\n",
    "print(\"Index:\\n\", collection.get()[\"ids\"][id])\n",
    "print(\"Text:\\n\", collection.get()[\"documents\"][id])\n",
    "print(\"Embedding vector:\\n\", collection.get(include=[\"embeddings\"])[\"embeddings\"][id])\n",
    "print(\"Metadata:\\n\", collection.get()[\"metadatas\"][id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Récupération des articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une requête est faite à la base de données vectorielle : en lui donnant une question, ou une phrase sur ce que l'on veut obtenir, la requête va nous retourner les chunks les plus pertinent (autrement dit les vecteurs les plus proches du vecteur de notre requête)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = collection.query(\n",
    "    query_texts=[\"What is the classical sample preparation method to see microstructure of steel ?\"], #La question que l'on souhaite poser\n",
    "    n_results=10 #Le nombre de résultats pertinents que l'on souhaite\n",
    ")\n",
    "\n",
    "Sources = list(set([i[\"source\"] for i in results[\"metadatas\"][0]]))\n",
    "Distances = [results[\"distances\"][0][[i[\"source\"] for i in results[\"metadatas\"][0]].index(i)] for i in Sources]\n",
    "Tri = [(x,y) for y,x in sorted(zip(Distances,Sources))] #Classement des résultats selon la distance : plus elle est petite, plus l'élément est pertinent\n",
    "\n",
    "for i in Tri:\n",
    "    print(f\" Source : {i[0]}  Distance : {i[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversion de la base de données vectorielle pour une utilisation avec un LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vdb = Chroma(client=client,collection_name=\"Articles\",persist_directory=\"db/\") #Récupération de la collection, mise au bon format\n",
    "vdb.persist()\n",
    "retriever = vdb.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 10})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test de requête pour vérifier le fonctionnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the classical sample preparation method to see microstructure of steel\"\n",
    "contexts = retriever.invoke(query)\n",
    "list(set([contexts[i].metadata[\"source\"] for i in range(len(contexts))]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Génération de réponse avec ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiation du chat avec GPT4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Création d'un modèle qui va utiliser les variables \"context\" et \"question\" pour faire une requête à ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use five sentences maximum and keep the answer as concise as possible.\n",
    "Always say \"thanks for asking!\" at the end of the answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "rag_prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Création de notre RAG : L'argument passé en entrée sera la question, et il utilisera le retriever pour faire la requête à la base de données vectorielle. Il utilisera le prompt créé plus tôt pour faire la requête à chatGPT via l'objet llm instancié plus tôt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the classical sample preparation method to see microstructure of steel ?\"\n",
    "answer = rag_chain.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
